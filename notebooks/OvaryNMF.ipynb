{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# go up one directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from functions import cosmic_val\n",
    "from models.nmf import nmf, refit_NMF\n",
    "from functions import cosmic_val\n",
    "from functions import data_handling as dh\n",
    "\n",
    "# set seed\n",
    "# np.random.seed(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/catalogues_Ovary_SBS.tsv\"\n",
    "cosmic_path = \"data/COSMIC_v3.4_SBS_GRCh37.txt\"\n",
    "output_folder = \"data/processed\"\n",
    "output_filename = \"Ordered_Ovary_SBS.csv\"\n",
    "ordered_data_path = os.path.join(output_folder, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists in  data/processed/Ordered_Ovary_SBS.csv\n"
     ]
    }
   ],
   "source": [
    "dh.load_preprocess_data(data_path, cosmic_path, sep1 = \"\\t\", sep2 = \"\\t\", output_folder = output_folder, output_filename = output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(ordered_data_path, index_col = 0)\n",
    "cosmic = pd.read_csv(cosmic_path, sep = \"\\t\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 4\n",
    "TOLERANCE = 1e-4\n",
    "MAX_ITERATIONS = 100_000_000\n",
    "SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, Loss: 16069.667324872784\n",
      "Iteration: 2000, Loss: 15522.852279786088\n",
      "Iteration: 3000, Loss: 13466.912904698695\n",
      "Iteration: 4000, Loss: 13341.96480095975\n",
      "Iteration: 5000, Loss: 13323.642686156858\n",
      "Iteration: 6000, Loss: 13266.956068467742\n",
      "Iteration: 7000, Loss: 13254.493504144322\n",
      "Iteration: 8000, Loss: 13026.57161210437\n",
      "Iteration: 9000, Loss: 13019.97565646557\n",
      "Iteration: 10000, Loss: 13014.706351306673\n",
      "Iteration: 11000, Loss: 13008.337430705345\n",
      "Iteration: 12000, Loss: 13006.195188787231\n",
      "Iteration: 13000, Loss: 13005.772654343444\n",
      "Iteration: 14000, Loss: 13005.613552920977\n",
      "Iteration: 15000, Loss: 13004.8584231583\n",
      "Iteration: 16000, Loss: 13004.381392278832\n",
      "Iteration: 17000, Loss: 13002.282632992843\n",
      "Iteration: 18000, Loss: 13002.014815982107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     15\u001b[0m signatures_nmf, exposures_nmf, loss_nmf \u001b[38;5;241m=\u001b[39m nmf(catalog_matrix \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mto_numpy(),\n\u001b[1;32m     16\u001b[0m                                               num_sign \u001b[38;5;241m=\u001b[39m LATENT_DIM,\n\u001b[1;32m     17\u001b[0m                                               tol \u001b[38;5;241m=\u001b[39m TOLERANCE,\n\u001b[1;32m     18\u001b[0m                                               max_iter \u001b[38;5;241m=\u001b[39m MAX_ITERATIONS)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Refitting NMF (test data)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m losses_refit \u001b[38;5;241m=\u001b[39m \u001b[43mrefit_NMF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures_nmf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m losses_test\u001b[38;5;241m.\u001b[39mappend(losses_refit[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculating signatures and exposures for NMF\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/DeepLearningProject/models/nmf.py:87\u001b[0m, in \u001b[0;36mrefit_NMF\u001b[0;34m(catalog_matrix, signature_matrix, tol, max_iter)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Compute the loss (Frobenius norm squared)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(catalog_matrix \u001b[38;5;241m-\u001b[39m signature_matrix\u001b[38;5;129m@E\u001b[39m, \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m)\n\u001b[1;32m     89\u001b[0m diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     90\u001b[0m n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_test = []\n",
    "signatures = []\n",
    "iterations = 1\n",
    "\n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "\n",
    "\n",
    "    # Extract train and test data\n",
    "    train_data = data.sample(frac = SPLIT, axis = 1)\n",
    "    test_data = data.drop(train_data.columns, axis = 1)\n",
    "    \n",
    "    # Applying NMF\n",
    "    signatures_nmf, exposures_nmf, loss_nmf = nmf(catalog_matrix = train_data.to_numpy(),\n",
    "                                                  num_sign = LATENT_DIM,\n",
    "                                                  tol = TOLERANCE,\n",
    "                                                  max_iter = MAX_ITERATIONS)\n",
    "    \n",
    "\n",
    "    # Refitting NMF (test data)\n",
    "\n",
    "    losses_refit = refit_NMF(test_data.to_numpy(), signatures_nmf)\n",
    "    losses_test.append(losses_refit[-1])\n",
    "\n",
    "    # Calculating signatures and exposures for NMF\n",
    "    diagonals_nmf = signatures_nmf.sum(axis=0)\n",
    "    exposures_nmf = exposures_nmf.T @ np.diag(diagonals_nmf)\n",
    "    signatures_nmf = (signatures_nmf) @ np.diag(1 / diagonals_nmf)\n",
    "    \n",
    "    losses_train.append(loss_nmf[-1])\n",
    "    signatures.append(signatures_nmf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Losses train: \", np.mean(losses_train))\n",
    "print(\"Losses test: \", np.mean(losses_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signatures = np.hstack(signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam = KMedoids(n_clusters = LATENT_DIM, metric='cosine').fit(all_signatures.T)\n",
    "consensus_signatures = all_signatures[:, pam.medoid_indices_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_signatures, mean_similarity = cosmic_val.compute_match(consensus_signatures, cosmic, index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_signatures.head())\n",
    "print(\"\\nMean similarity of the matched signatures: \", mean_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_GENOMICS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
