{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f74a4566450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# go up one directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.decomposition import PCA\n",
    "from functions.cosmic_val import *\n",
    "from functions.graph_tools import *\n",
    "from functions.data_handling import data_augmentation\n",
    "from models.muse import *\n",
    "from functions import cosmic_val\n",
    "from functions import data_handling as dh\n",
    "from tqdm import tqdm\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# # set seed\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/catalogues_Ovary_SBS.tsv\"\n",
    "cosmic_path = \"data/COSMIC_v3.4_SBS_GRCh37.txt\"\n",
    "output_folder = \"data/processed\"\n",
    "output_filename = \"Ordered_Ovary_SBS.csv\"\n",
    "ordered_data_path = os.path.join(output_folder, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists in  data/processed/Ordered_Ovary_SBS.csv\n"
     ]
    }
   ],
   "source": [
    "dh.load_preprocess_data(data_path, cosmic_path, sep1 = \"\\t\", sep2 = \"\\t\", output_folder = output_folder, output_filename = output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(ordered_data_path, index_col = 0)\n",
    "cosmic = pd.read_csv(cosmic_path, sep = \"\\t\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_ONE = 128\n",
    "TOLERANCE = 1e-10\n",
    "CONSTRAINT = 'identity'\n",
    "\n",
    "augmented_data = data_augmentation(X=data, augmentation=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 26150)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(augmented_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of lists: {k: [iterations]}\n",
    "results_dict = defaultdict(list)\n",
    "\n",
    "losses_train = []\n",
    "signatures = []\n",
    "iterations = 10\n",
    "k_range = 10\n",
    "\n",
    "augmented_data = data_augmentation(X=data, augmentation=50)\n",
    "\n",
    "for k in tqdm(range(3, k_range)):\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        muse_model = HybridAutoencoder(input_dim=data.shape[0], \n",
    "                                        l_1=L_ONE,\n",
    "                                        latent_dim=k,\n",
    "                                        weights = 'xavier',)\n",
    "\n",
    "        # Training MUSE\n",
    "        muse_error, muse_signatures, muse_exposures, muse_train_loss, muse_val_loss = train_model_for_extraction(\n",
    "            model=muse_model,\n",
    "            X_aug_multi_scaled=augmented_data.T,\n",
    "            X_scaled=data.T,\n",
    "            signatures=k,\n",
    "            epochs=1000,\n",
    "            batch_size=64,\n",
    "            save_to='muse_test',\n",
    "            iteration=i,\n",
    "            patience=15,\n",
    "            beta = 0.01,\n",
    "            lr = 0.001\n",
    "        )\n",
    "\n",
    "        # Normalize signatures\n",
    "        diagonals_muse = muse_signatures.sum(axis=0)\n",
    "        muse_exposures = muse_exposures.T @ np.diag(diagonals_muse)\n",
    "        muse_signatures = muse_signatures @ np.diag(1 / diagonals_muse)\n",
    "\n",
    "        # Store results\n",
    "        losses_train.append(muse_train_loss)\n",
    "        signatures.append(muse_signatures)\n",
    "\n",
    "        # Store data in structured format\n",
    "        results_dict[k].append({\n",
    "            \"iteration\": i,\n",
    "            \"muse_error\": muse_error, # This is the reconstruction error\n",
    "            \"muse_signatures\": muse_signatures  # Keep as NumPy array for easier processing\n",
    "        })\n",
    "\n",
    "    # Convert dictionary into a DataFrame for better analysis\n",
    "    df_results = pd.DataFrame([\n",
    "    {\"k\": k, \"iteration\": entry[\"iteration\"], \"muse_error\": entry[\"muse_error\"], \"muse_signatures\": entry[\"muse_signatures\"]}\n",
    "    for k, entries in results_dict.items()\n",
    "    for entry in entries\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = find_best_k(df_results, avg_threshold= 0.5, min_threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Optimize hyperparams & then run bunch of times. Adam lr & beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to minimize (muse_error)\n",
    "def objective(params):\n",
    "    beta, lr = params\n",
    "    \n",
    "    muse_model = HybridAutoencoder(\n",
    "        input_dim=data.shape[0],  # 96\n",
    "        l_1=L_ONE,\n",
    "        latent_dim=k,  # Best k found earlier\n",
    "        weights='xavier',\n",
    "    )\n",
    "\n",
    "    # Train model with candidate beta and learning rate\n",
    "    muse_error, _, _, _, _ = train_model_for_extraction(\n",
    "        model=muse_model,\n",
    "        X_aug_multi_scaled=augmented_data.T,\n",
    "        X_scaled=data.T,\n",
    "        signatures=k, # Best k found earlier\n",
    "        epochs=1000,\n",
    "        batch_size=64,\n",
    "        save_to='muse_test',\n",
    "        iteration=0,  # Single run per candidate\n",
    "        patience=15,\n",
    "        lr=lr,  # Optimized learning rate\n",
    "        beta=beta  # Optimized beta\n",
    "    )\n",
    "    \n",
    "    return muse_error  # Lower is better\n",
    "\n",
    "# Define search space\n",
    "search_space = [\n",
    "    Real(1e-4, 1e-2, name=\"beta\"), \n",
    "    Real(1e-4, 1e-2, name=\"lr\")\n",
    "]\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "result = gp_minimize(objective, search_space, n_calls=20, random_state=123)\n",
    "\n",
    "# Best hyperparameters found\n",
    "best_beta, best_lr = result.x\n",
    "print(f\"Optimal beta: {best_beta}, Optimal lr: {best_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MUSE with optimized hyperparameters\n",
    "results_dict = defaultdict(list)\n",
    "\n",
    "iterations = 30\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    muse_model = HybridAutoencoder(\n",
    "        input_dim=data.shape[0],  # 96\n",
    "        l_1=L_ONE,\n",
    "        latent_dim=k,  # Best k found earlier\n",
    "        weights='xavier',\n",
    "    )\n",
    "\n",
    "    muse_error, muse_signatures, muse_exposures, muse_train_loss, muse_val_loss = train_model_for_extraction(\n",
    "        model=muse_model,\n",
    "        X_aug_multi_scaled=augmented_data.T,\n",
    "        X_scaled=data.T,\n",
    "        signatures=k, # Best k found earlier\n",
    "        epochs=2000,\n",
    "        batch_size=64,\n",
    "        save_to='muse_test',\n",
    "        iteration=0,  # Single run\n",
    "        patience=15,\n",
    "        lr=best_lr,  # Optimized learning rate\n",
    "        beta=best_beta  # Optimized beta\n",
    "    )\n",
    "\n",
    "    # Normalize signatures\n",
    "\n",
    "    diagonals_muse = muse_signatures.sum(axis=0)\n",
    "    muse_exposures = muse_exposures.T @ np.diag(diagonals_muse)\n",
    "    muse_signatures = muse_signatures @ np.diag(1 / diagonals_muse)\n",
    "\n",
    "    # Store results\n",
    "\n",
    "    losses_train.append(muse_train_loss)\n",
    "    signatures.append(muse_signatures)\n",
    "\n",
    "    # Store data in structured format\n",
    "\n",
    "    results_dict[k].append({\n",
    "        \"iteration\": i,\n",
    "        \"muse_error\": muse_error,  # This is the reconstruction error\n",
    "        \"muse_signatures\": muse_signatures  # Keep as NumPy array for easier processing\n",
    "    })\n",
    "\n",
    "    # Convert dictionary into a DataFrame for better analysis\n",
    "\n",
    "df_results = pd.DataFrame([\n",
    "    {\"k\": k, \"iteration\": entry[\"iteration\"], \"muse_error\": entry[\"muse_error\"], \"muse_signatures\": entry[\"muse_signatures\"]}\n",
    "    for k, entries in results_dict.items()\n",
    "    for entry in entries\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = df_results[df_results['k'] == k]['muse_signatures'].values # Get signatures\n",
    "\n",
    "all_signatures = np.hstack(signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam = KMedoids(n_clusters = k, metric='cosine').fit(all_signatures.T)\n",
    "labels = pam.labels_\n",
    "medoid_indices = pam.medoid_indices_\n",
    "consensus_signatures = all_signatures[:, medoid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_signatures, mean_similarity = cosmic_val.compute_match(consensus_signatures, cosmic, index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_signatures)\n",
    "print(\"\\nMean similarity of the matched signatures: \", mean_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = cosmic_val.compute_all_matches(all_signatures, cosmic, k = k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine_similarity_matrix(all_matches, title = \"Cosine similarity matrix non-linear AE\", figsize=(24,12), legend_colums = 4, palette = 'tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signature_frequency(all_matches, title = \"Signature frequency non-linear AE\", figsize=(12,7), palette = 'tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_signatures = PCA(n_components=2).fit_transform(all_signatures.T)\n",
    "plot_clusters(reduced_signatures, labels, medoid_indices, k, \"non-linear AE signature clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consensus = pd.DataFrame(consensus_signatures, index = data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signature(df_consensus, \"non-linear AE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_GENOMICS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
